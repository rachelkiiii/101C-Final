import numpy as np
import nltk
import pandas as pd

from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from spellchecker import SpellChecker
import string
import time


nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

start = time.time()
df = pd.read_csv("~/Desktop/IMDB Dataset.csv")
df= df.iloc[0:100]
# Tokenize the 'review' column
df['review'] = df['review'].apply(lambda review: word_tokenize(review))

# Define stop words and punctuation characters
stop_words = set(stopwords.words('english'))
punctuation_chars = set(string.punctuation)


# Remove stop words and punctuation from tokenized reviews

df['review'] = df['review'].apply(lambda tokens: [word for word in tokens if word.isalpha() and word.lower() not in stop_words and word not in punctuation_chars and word.lower() not in {'br','\'\'','\'s', '``', 'oz'}])

# Initialize spellchecker
spell = SpellChecker()

def process_review(tokens):
    new_sentence = []
    for word, tag in pos_tag(tokens):
        word = word.lower()
        if tag == 'RB':  # Checking if the word is an adverb
            # Perform adverb to adjective conversion
            if word.endswith('ly'):  # Check if the adverb ends with 'ly'
                new_word = word[:-2]  # Remove 'ly' and add 'e' for demonstration
                new_sentence.append(spell.correction(new_word))  # Spell check the new word
            else:
                new_sentence.append(spell.correction(word))  # Spell check the word as is
        else:
            new_sentence.append(spell.correction(word))  # Spell check non-adverbs

    # Remove None values after spell checking
    filtered_list = [value for value in new_sentence if value is not None]

    return filtered_list

# Apply the processing function to each row
df['review'] = df['review'].apply(process_review)


end = time.time()

elapsed_time = end-start
print(elapsed_time)

print(df)

# Specify the file path and name for the CSV file
csv_file_path = '~/Downloads/preprocessed.csv'

# Save the DataFrame to a CSV file
df.to_csv(csv_file_path, index=False)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
df = pd.read_csv("/Users/kaylinlee/Desktop/cleandata.csv")
X = df['review']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_tfidf, y_train)
y_pred = rf_classifier.predict(X_test_tfidf)


# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)

# Classification report
print(classification_report(y_test, y_pred))

# Confusion matrix
print(confusion_matrix(y_test, y_pred))

# ROC Curve
y_test_binary = y_test.map({'negative': 0, 'positive': 1})
y_score = rf_classifier.predict_proba(X_test_tfidf)[:, 1]
fpr, tpr, _ = roc_curve(y_test_binary, y_score)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest Model ROC Curve')
plt.legend(loc='lower right')
plt.show()
